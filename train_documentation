
This document contains TOLO + DINO integration docs

# ------------------- 1. Imports -------------------
import os, glob, shutil, random, yaml, torch
import torchvision.transforms as T
from PIL import Image

---

Define dataset directories.
DATASET_DIR → input dataset (folders = classes)
OUTPUT_DIR → where the YOLO-ready dataset will be stored
# ------------------- 2. Configurations -------------------
DATASET_DIR = "dataset_limited"
OUTPUT_DIR = "dataset_final"
os.makedirs(OUTPUT_DIR, exist_ok=True)

---

3. Model + Transform

Load the pretrained DINO model from Facebook Research using torch.hub.
We also define the preprocessing pipeline for input images (resize, crop, normalize).
# ------------------- 3. Model + Transform -------------------
model = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16').eval()
transform = T.Compose([
    T.Resize(224), T.CenterCrop(224), T.ToTensor(),
    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
])

---

4. Collect Classes

Read all subfolders inside the dataset.
Each folder corresponds to a class, and we map classes → numeric IDs.
# ------------------- 4. Classes -------------------
classes = sorted([d for d in os.listdir(DATASET_DIR) if os.path.isdir(os.path.join(DATASET_DIR, d))])
class_to_idx = {cls: i for i, cls in enumerate(classes)}
print("✅ Classes:", class_to_idx)

---

***

---
TRAIN BY:

import subprocess

# Command to run YOLOv5 training
command = [
    "python", "yolov5/train.py",
    "--img", "640",
    "--batch", "16",
    "--epochs", "3",
    "--data", "yolov5/data/customD.yaml",
    "--weights", "yolov5s.pt",
    "--cache",
    "--workers", "0"
]

# Execute the command
subprocess.run(command)
---

or, debug by:

---
import subprocess

command = [
    "python", "yolov5/detect.py",
    "--weights", "yolov5/runs/train/exp4/weights/best.pt",
    "--img", "640",
    "--conf", "0.25",
    "--source", "a.jpg"  # folder or image path
]

# Run the command
subprocess.run(command)
# Get latest output image path

---
***

5. Collect & Split Dataset

Collect all image paths and their class IDs.
Shuffle the dataset and split into 80% train and 20% validation.
# ------------------- 5. Collect + Split -------------------
all_images = [(p, class_to_idx[cls]) 
              for cls in classes 
              for p in glob.glob(os.path.join(DATASET_DIR, cls, "*.*"))]
random.shuffle(all_images)
split_idx = int(0.8 * len(all_images))
splits = {"train": all_images[:split_idx], "val": all_images[split_idx:]}

---

6. Process & Save Images

For each image:

Load and preprocess.

Use DINO attention map to generate a bounding box.

Convert bbox into YOLO format (normalized center, width, height).

Copy the image and save its label .txt file.
# ------------------- 6. Processing -------------------
def process_and_save(images, split):
    for img_path, cls_id in images:
        try:
            img = Image.open(img_path).convert("RGB"); w, h = img.size
            tensor = transform(img).unsqueeze(0)
            with torch.no_grad():
                attn = model.get_last_selfattention(tensor)[0,:,0,1:].mean(0).reshape(14,14).cpu()
            mask = T.Resize((h,w))(attn[None,None])[0,0] > attn.mean()
            coords = torch.nonzero(mask)
            if not len(coords): continue

            y_min, x_min = coords.min(0).values; y_max, x_max = coords.max(0).values
            x_c, y_c = ((x_min+x_max)/2)/w, ((y_min+y_max)/2)/h
            bw, bh = (x_max-x_min)/w, (y_max-y_min)/h

            out_img = os.path.join(OUTPUT_DIR, split, "images", os.path.basename(img_path))
            out_label = os.path.join(OUTPUT_DIR, split, "labels", os.path.splitext(os.path.basename(img_path))[0]+".txt")
            os.makedirs(os.path.dirname(out_img), exist_ok=True)
            os.makedirs(os.path.dirname(out_label), exist_ok=True)
            shutil.copy(img_path, out_img)
            with open(out_label, "w") as f: 
                f.write(f"{cls_id} {x_c:.6f} {y_c:.6f} {bw:.6f} {bh:.6f}\n")

        except Exception as e: 
            print(f"⚠️ {img_path}: {e}")

for split, imgs in splits.items(): 
    process_and_save(imgs, split)

---

***
resize:
import os
import shutil
import random

# -----------------------------
# User Settings
# -----------------------------
SOURCE_DIR = "archive"              # original dataset
OUTPUT_DIR = "dataset_limited"      # final dataset folder
TARGET_SIZE_MB = 300                # target size in MB
# -----------------------------

TARGET_SIZE = TARGET_SIZE_MB * 1024 * 1024  # MB → bytes

# Step 1: Collect images per class
class_images = {}
for class_name in os.listdir(SOURCE_DIR):
    class_dir = os.path.join(SOURCE_DIR, class_name)
    if not os.path.isdir(class_dir):
        continue
    images = []
    for img_name in os.listdir(class_dir):
        img_path = os.path.join(class_dir, img_name)
        if os.path.isfile(img_path):
            size = os.path.getsize(img_path)
            images.append((img_path, size))
    if images:
        class_images[class_name] = images

num_classes = len(class_images)

# Step 2: Distribute budget equally across classes
budget_per_class = TARGET_SIZE // num_classes

# Step 3: Select equal number of images per class (as close as possible)
selected = []
final_total_size = 0

# Find min number of images across all classes
min_images = min(len(imgs) for imgs in class_images.values())

for class_name, images in class_images.items():
    random.shuffle(images)

    class_selected = []
    size_so_far = 0
    for img_path, size in images:
        if size_so_far + size > budget_per_class:
            break
        class_selected.append(img_path)
        size_so_far += size

    # In case some classes have much fewer images, restrict others too
    if len(class_selected) > min_images:
        class_selected = class_selected[:min_images]
        size_so_far = sum(os.path.getsize(p) for p in class_selected)

    selected.append((class_name, class_selected, size_so_far))
    final_total_size += size_so_far

# Step 4: Copy selected images
for class_name, class_selected, _ in selected:
    dest_dir = os.path.join(OUTPUT_DIR, class_name)
    os.makedirs(dest_dir, exist_ok=True)
    for img_path in class_selected:
        shutil.copy(img_path, os.path.join(dest_dir, os.path.basename(img_path)))

# Summary
print("✅ Final dataset created at:", OUTPUT_DIR)
print("Total size:", round(final_total_size / 1024 / 1024, 2), "MB")
for class_name, _, class_size in selected:
    print(f"  {class_name}: {round(class_size/1024/1024,2)} MB, {len(_)} images")

***

7. Create data.yaml

Finally, we generate a data.yaml file that YOLO requires.
It contains:

Training image path

Validation image path

Number of classes

Class names
# ------------------- 7. data.yaml -------------------
yaml.dump({
    "train": os.path.join(OUTPUT_DIR, "train/images"),
    "val": os.path.join(OUTPUT_DIR, "val/images"),
    "nc": len(classes), "names": classes
}, open(os.path.join(OUTPUT_DIR, "data.yaml"), "w"))

